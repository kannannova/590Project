{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 590 Topic 8 Assignment\n",
    "## Milestone 3\n",
    "## Kannan Nova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Details : \n",
    "The warehouse is a part of supply chain logistics industry where store the goods and bring back it from storage for shipment.  Warehouse has two major core processes such as \n",
    "Put away where download the goods from truck and place it into final location inside warehouse (McCrea, 2016). \n",
    "Picking where take out the goods from final location and stuff into truck for shipping (Lopienski, 2020).\n",
    "These two warehouse operations are overly complex and more labor-oriented tasks. The workers manually go there and pick the goods if it is a small and light weight otherwise, they operate forklifts for picking. Sometimes robots are used for picking (Gillman, 2016). If the system is not providing the proper optimal route/shortest path details for picking a particular order, then workers spend more time on walking instead of performing the actual picking job.\n",
    "Based on literature reviews, the picking process is optimized by linearly or deterministic or deep learning, but it is dynamic and stochastic because goods can be placed anywhere in warehouse during put away/storing (Janse, 2019).  Thus, these enable to use reinforcement learning (RL) and if system is not providing the shortest optimal path for picking operation, then the most of times, workers spend more time on walking on warehouse instead performing the actual task. \n",
    "This project is based on dynamic environment and if picking process failed, system/agent will learn it from failure for future improvement and it needs the data for environment, state, Q-table, and locations. \n",
    "\n",
    "This project is developed by using reinformcement Q-learning algorithm through python language.\n",
    "The warehouse layout location details will be used as states for q-learning. It has three files such as wms.py/580_topic5_milestone3.py, app.py and index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data From File for creating states for reinforcement Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Nova/Personal/GCU-20210416T232635Z-001/GCU/Admission and course/projthesis/wmscode/wmsdata.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no null and missing value so there is no need of data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need of checking corelation and multivarinace because it is not supervised or unsupervised learning.\n",
    "It has three types of daata such states data, rewards, and actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "* The panda dataframe df data for states has to be converted into numpy array\n",
    "* QTable has to created and initialized with zeroes\n",
    "* Action data has to be created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "envRows = 30\n",
    "envColumns = 30\n",
    "rewards = df.to_numpy() \n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "qValues = np.zeros((envRows, envColumns, 4)) # 4 represents number of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.box(grid='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLWMS:\n",
    "    rewards = df.to_numpy() \n",
    "    num_rows, num_cols = rewards.shape\n",
    "    envRows = num_cols\n",
    "    envColumns = num_cols\n",
    "    qValues = np.zeros((envRows, envColumns, 4))\n",
    "    actions = ['up', 'right', 'down', 'left']    \n",
    "    cumulative_rewards=[]\n",
    "    #This method returns current row and column, initiallly it returns a random row and column\n",
    "    def findStartLocation(self):\n",
    "        current_row_index = np.random.randint(self.envRows)\n",
    "        current_column_index = np.random.randint(self.envColumns)\n",
    "        while self.isTerminalOrWall(current_row_index, current_column_index):\n",
    "            current_row_index = np.random.randint(self.envRows)\n",
    "            current_column_index = np.random.randint(self.envColumns)\n",
    "        return current_row_index, current_column_index\n",
    "    \n",
    "    #This method returns whether state is terminal/wall or not\n",
    "    def isTerminalOrWall(self,current_row_index, current_column_index):\n",
    "        if self.rewards[current_row_index, current_column_index] == -5.:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    #This method returns the next action based on greedy algorithm\n",
    "    def findNextAction(self,current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.qValues[current_row_index, current_column_index])\n",
    "        else: #choose a random action\n",
    "            return np.random.randint(4)\n",
    "    \n",
    "     #This method returns the next location based on action taken\n",
    "    def findNextLocation(self,current_row_index, current_column_index, action_index):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.envColumns - 1:\n",
    "            new_column_index += 1\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.envRows - 1:\n",
    "            new_row_index += 1\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        return new_row_index, new_column_index\n",
    "    \n",
    "    #This method returns the best shortest optimal path from staging area to picking location. \n",
    "    def findShortestPath(self,start_row_index, start_column_index):\n",
    "        if self.isTerminalOrWall(start_row_index, start_column_index):\n",
    "            return []\n",
    "        else: \n",
    "            current_row_index, current_column_index = start_row_index, start_column_index\n",
    "            shortestPath = []\n",
    "            shortestPath.append([current_row_index, current_column_index])\n",
    "            while not self.isTerminalOrWall(current_row_index, current_column_index):\n",
    "                self.action_index = self.findNextAction(current_row_index, current_column_index, 1.)\n",
    "                current_row_index, current_column_index = self.findNextLocation(current_row_index, current_column_index, self.action_index)\n",
    "                shortestPath.append([current_row_index, current_column_index])\n",
    "            return shortestPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "discount_factor = 0.9\n",
    "learning_rate = 0.9\n",
    "epsilon = 0.9 \n",
    "num_episodes = 5000\n",
    "episode_lengths = np.zeros(num_episodes)\n",
    "episode_rewards = np.zeros(num_episodes)\n",
    "total = 0\n",
    "deltaStates = []\n",
    "\n",
    "wms = RLWMS()\n",
    "for episode in range(num_episodes): #20000 episodes\n",
    "    row_index, column_index = wms.findStartLocation()\n",
    "    # if it is terminal location then it returns immediately\n",
    "    while not wms.isTerminalOrWall(row_index, column_index):\n",
    "        wms.action_index = wms.findNextAction(row_index, column_index, epsilon)  \n",
    "        old_row_index, old_column_index = row_index, column_index \n",
    "        row_index, column_index = wms.findNextLocation(row_index, column_index, wms.action_index) \n",
    "        wms.reward = wms.rewards[row_index, column_index]\n",
    "        \n",
    "        \n",
    "        total += wms.reward\n",
    "        wms.cumulative_rewards.append(total)        \n",
    "        # Update statistics\n",
    "        episode_rewards[episode] += wms.reward\n",
    "        episode_lengths[episode] += total  \n",
    "        \n",
    "        old_q_value = wms.qValues[old_row_index, old_column_index, wms.action_index]\n",
    "        temporal_difference = wms.reward + (discount_factor * np.max(wms.qValues[row_index, column_index])) - old_q_value    \n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        wms.qValues[old_row_index, old_column_index, wms.action_index] = new_q_value\n",
    "        \n",
    "        deltaStates.append(new_q_value)\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = wms.findShortestPath(28,6)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_specific_cell1(x):\n",
    "    color = 'background-color : green'\n",
    "    df1 = pd.DataFrame('',index=x.index, columns=x.columns)\n",
    "    #path = wms.get_shortest_path(6,2)\n",
    "    for each in path:        \n",
    "        df1.loc[each[0],each[1]]= color\n",
    "    return df1\n",
    "df = pd.DataFrame(wms.rewards)\n",
    "df.style.apply(color_specific_cell1, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wms.qValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wms.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile = open(\"D:/Nova/Personal/GCU-20210416T232635Z-001/GCU/Admission and course/projthesis/wmscode/rlwms.pkl\", 'wb')\n",
    "#pickle the dictionary and write it to file\n",
    "pickle.dump(wms, picklefile)\n",
    "#close the file\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating the deployment package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmsUnPicle = joblib.load(\"D:/Nova/Personal/GCU-20210416T232635Z-001/GCU/Admission and course/projthesis/wmscode/rlwms.pkl\")\n",
    "pathUnPicke = wmsUnPicle.findShortestPath(25,17)\n",
    "pathUnPicke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_specific_cell(x):\n",
    "    color = 'background-color : green'\n",
    "    df1 = pd.DataFrame('',index=x.index, columns=x.columns)    \n",
    "    for each in pathUnPicke:        \n",
    "        df1.loc[each[0],each[1]]= color\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wmsUnPicle.rewards)\n",
    "df.style.apply(color_specific_cell, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* Warehouse layout location data is used for states.\n",
    "* This model uses reinforcement Q-learning algorithm and finds out the shortest optimal path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
